{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7aea94-07d0-41bc-88f6-fb8316da119a",
   "metadata": {
    "id": "7d7aea94-07d0-41bc-88f6-fb8316da119a"
   },
   "source": [
    "# Machine learning Work Loads Implemented for scheduling algorithms\n",
    "\n",
    "## Task 1: MNIST Classification\n",
    "## Task 2:\n",
    "## Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61f607-b62f-4250-aee1-c830018a4661",
   "metadata": {
    "id": "cd61f607-b62f-4250-aee1-c830018a4661"
   },
   "outputs": [],
   "source": [
    "def mnist_classification_task():\n",
    "    # Load and preprocess the MNIST dataset\n",
    "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "    train_images = train_images.reshape((60000, 28, 28, 1))  # Reshape to 28x28x1\n",
    "    test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "    train_images = train_images.astype('float32') / 255  # Normalize pixel values\n",
    "    test_images = test_images.astype('float32') / 255\n",
    "\n",
    "    #Build a simple CNN model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))  # 10 output classes for digits 0-9\n",
    "\n",
    "    #Training the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_split=0.1)  #Change number of epochs for faster/slower training\n",
    "\n",
    "    #Evalutation\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print(f'Task 1: MNIST Classification - Test accuracy: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TBtNdWdnKZEp",
   "metadata": {
    "id": "TBtNdWdnKZEp"
   },
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e90c7f-6883-4ca8-bc6f-1794b8acaabe",
   "metadata": {
    "id": "e0e90c7f-6883-4ca8-bc6f-1794b8acaabe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)  # 10 input features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)  # 3 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create random data for training\n",
    "def generate_data():\n",
    "    # 1000 samples, 10 features per sample\n",
    "    X = torch.rand(1000, 10)\n",
    "    # Labels for 3 classes (0, 1, or 2)\n",
    "    y = torch.randint(0, 3, (1000,))\n",
    "    return X, y\n",
    "\n",
    "# Training function\n",
    "def train_model(model, X, y, epochs=5, learning_rate=0.001):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Move model and data to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "        outputs = model(X)     # Forward pass\n",
    "        loss = criterion(outputs, y)  # Compute loss\n",
    "        loss.backward()  # Backward pass (compute gradients)\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Main code to run the training\n",
    "def main():\n",
    "    # Generate random training data\n",
    "    X, y = generate_data()\n",
    "\n",
    "    # Create a model instance\n",
    "    model = SimpleModel()\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, X, y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CyUJOOkKW4vV",
   "metadata": {
    "id": "CyUJOOkKW4vV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate random sequence data\n",
    "def generate_sequence_data(num_samples=1000, sequence_length=10, num_classes=3):\n",
    "    # Generate random sequences (each sample has 'sequence_length' features)\n",
    "    X = np.random.rand(num_samples, sequence_length, 1)  # Shape: (samples, timesteps, features)\n",
    "    # Generate random labels (3-class classification)\n",
    "    y = np.random.randint(0, num_classes, num_samples)\n",
    "    return X, y\n",
    "\n",
    "# Build LSTM model\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape, return_sequences=False))  # LSTM layer with 64 units\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer with softmax activation for classification\n",
    "    return model\n",
    "\n",
    "# Train the LSTM model\n",
    "def train_lstm_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)\n",
    "    return history\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Generate the dataset\n",
    "    X, y = generate_sequence_data(num_samples=1000, sequence_length=10, num_classes=3)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create LSTM model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (timesteps, features)\n",
    "    num_classes = 3\n",
    "    model = create_lstm_model(input_shape, num_classes)\n",
    "\n",
    "    # Train the model\n",
    "    history = train_lstm_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate the model on validation data\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2a6e1-f45d-4085-83d9-1cb0bad0a886",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c157835d-727f-4628-83b4-4e7a372413b6",
   "metadata": {
    "id": "c157835d-727f-4628-83b4-4e7a372413b6"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "def rf_task():\n",
    "    # Create a synthetic classification dataset\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create a Random Forest classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "    # Train the model (this would be treated as one job/task)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Evaluate the model\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Random Forest Classifier Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a97c6e-c33a-4b8b-965a-dcca8454d1c2",
   "metadata": {
    "id": "04a97c6e-c33a-4b8b-965a-dcca8454d1c2"
   },
   "outputs": [],
   "source": [
    "def fcfs_scheduler(task_list):\n",
    "    print(\"Scheduler: Starting tasks in First-Come, First-Served order...\\n\")\n",
    "    for task in task_list:\n",
    "        task()  #However we input the task list, thats the order they will be completed.\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #List of Machine Learning Workloads\n",
    "    tasks = [mnist_classification_task, task2, task3]\n",
    "\n",
    "    #Run the scheduler with the task list\n",
    "    fcfs_scheduler(tasks)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
